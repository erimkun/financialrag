import pdfplumber
import pdf2image
from pathlib import Path
import json
from PIL import Image
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from typing import Dict, List, Optional, Tuple, Any
import logging

# Logging setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HybridPDFExtractor:
    """
    Hybrid PDF Extraction System - Context7 Best Practice
    Ã‡oklu araÃ§ kullanarak cross-validation ve consensus-based extraction
    """
    
    def __init__(self, pdf_path: str, output_dir: str = "extracted_data"):
        self.pdf_path = Path(pdf_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Performance metrics
        self.performance_stats = {
            'pdfplumber': {'time': 0.0, 'success': 0, 'errors': 0},
            'pdf2image': {'time': 0.0, 'success': 0, 'errors': 0},
            'alternative': {'time': 0.0, 'success': 0, 'errors': 0}
        }
        
        logger.info(f"ğŸš€ Hybrid PDF Extractor baÅŸlatÄ±ldÄ±: {self.pdf_path}")
    
    def extract_text_pdfplumber(self) -> List[Dict]:
        """pdfplumber ile metin Ã§Ä±karÄ±mÄ±"""
        start_time = time.time()
        try:
            text_data = []
            with pdfplumber.open(self.pdf_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    text = page.extract_text() or ""
                    text_data.append({
                        "sayfa": i + 1,
                        "metin": text,
                        "paragraflar": [p.strip() for p in text.split('\n') if p.strip()],
                        "kaynak": "pdfplumber"
                    })
            
            self.performance_stats['pdfplumber']['time'] = time.time() - start_time
            self.performance_stats['pdfplumber']['success'] += 1
            logger.info(f"âœ… pdfplumber metin Ã§Ä±karÄ±mÄ±: {len(text_data)} sayfa")
            return text_data
            
        except Exception as e:
            self.performance_stats['pdfplumber']['errors'] += 1
            logger.error(f"âŒ pdfplumber hata: {e}")
            return []
    
    def extract_text_alternative(self) -> List[Dict]:
        """Alternatif metin Ã§Ä±karÄ±mÄ± (pdfplumber ile farklÄ± yÃ¶ntem)"""
        start_time = time.time()
        try:
            text_data = []
            with pdfplumber.open(self.pdf_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    # Alternatif: karakterleri de Ã§Ä±kar
                    text = page.extract_text() or ""
                    chars = page.chars
                    
                    # Karakterlerden ek bilgi Ã§Ä±kar
                    if chars:
                        font_sizes = [c.get('size', 0) for c in chars if c.get('size')]
                        avg_font_size = sum(font_sizes) / len(font_sizes) if font_sizes else 12
                    else:
                        avg_font_size = 12
                    
                    text_data.append({
                        "sayfa": i + 1,
                        "metin": text,
                        "paragraflar": [p.strip() for p in text.split('\n') if p.strip()],
                        "ortalama_font_boyutu": avg_font_size,
                        "kaynak": "pdfplumber_alternative"
                    })
            
            self.performance_stats['alternative']['time'] = time.time() - start_time
            self.performance_stats['alternative']['success'] += 1
            logger.info(f"âœ… Alternatif metin Ã§Ä±karÄ±mÄ±: {len(text_data)} sayfa")
            return text_data
            
        except Exception as e:
            self.performance_stats['alternative']['errors'] += 1
            logger.error(f"âŒ Alternatif metin hatasÄ±: {e}")
            return []
    
    def extract_tables_pdfplumber(self) -> List[Dict]:
        """pdfplumber ile tablo Ã§Ä±karÄ±mÄ±"""
        try:
            tables_data = []
            with pdfplumber.open(self.pdf_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    tables = page.extract_tables()
                    for j, table in enumerate(tables):
                        if table:
                            tables_data.append({
                                "sayfa": i + 1,
                                "tablo_id": j + 1,
                                "data": {"rows": table},
                                "kaynak": "pdfplumber"
                            })
            
            logger.info(f"âœ… pdfplumber tablo Ã§Ä±karÄ±mÄ±: {len(tables_data)} tablo")
            return tables_data
            
        except Exception as e:
            logger.error(f"âŒ pdfplumber tablo hatasÄ±: {e}")
            return []
    
    def extract_images_pdf2image(self) -> List[Dict]:
        """pdf2image + poppler ile gÃ¶rsel Ã§Ä±karÄ±mÄ±"""
        start_time = time.time()
        try:
            images_data = []
            # PDF'i yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼kte gÃ¶rsele Ã§evir
            pages = pdf2image.convert_from_path(
                str(self.pdf_path),  # Path'i string'e Ã§evir
                dpi=300,  # YÃ¼ksek kalite
                fmt='PNG'
            )
            
            for page_num, page_image in enumerate(pages):
                img_path = self.output_dir / f"page_{page_num + 1}_hq.png"
                page_image.save(img_path, optimize=True)
                
                images_data.append({
                    "sayfa": page_num + 1,
                    "gÃ¶rsel_path": str(img_path),
                    "Ã§Ã¶zÃ¼nÃ¼rlÃ¼k": "300dpi",
                    "kaynak": "pdf2image"
                })
            
            self.performance_stats['pdf2image']['time'] = time.time() - start_time
            self.performance_stats['pdf2image']['success'] += 1
            logger.info(f"âœ… pdf2image gÃ¶rsel Ã§Ä±karÄ±mÄ±: {len(images_data)} gÃ¶rsel")
            return images_data
            
        except Exception as e:
            self.performance_stats['pdf2image']['errors'] += 1
            logger.error(f"âŒ pdf2image hata: {e}")
            return []
    
    def extract_images_pdfplumber(self) -> List[Dict]:
        """pdfplumber ile gÃ¶rsel Ã§Ä±karÄ±mÄ± (fallback)"""
        try:
            images_data = []
            with pdfplumber.open(self.pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    img_path = self.output_dir / f"page_{page_num + 1}_standard.png"
                    page_img = page.to_image(resolution=150)
                    page_img.save(img_path)
                    
                    images_data.append({
                        "sayfa": page_num + 1,
                        "gÃ¶rsel_path": str(img_path),
                        "Ã§Ã¶zÃ¼nÃ¼rlÃ¼k": "150dpi",
                        "kaynak": "pdfplumber"
                    })
            
            logger.info(f"âœ… pdfplumber gÃ¶rsel Ã§Ä±karÄ±mÄ±: {len(images_data)} gÃ¶rsel")
            return images_data
            
        except Exception as e:
            logger.error(f"âŒ pdfplumber gÃ¶rsel hatasÄ±: {e}")
            return []
    
    def consensus_text_extraction(self, texts_pdfplumber: List[Dict], texts_alternative: List[Dict]) -> List[Dict]:
        """Cross-validation ile metin consensus"""
        consensus_texts = []
        
        for i in range(max(len(texts_pdfplumber), len(texts_alternative))):
            page_num = i + 1
            
            # Her iki kaynaktan da veri al
            text_plumber = texts_pdfplumber[i] if i < len(texts_pdfplumber) else None
            text_alternative = texts_alternative[i] if i < len(texts_alternative) else None
            
            if text_plumber and text_alternative:
                # Ä°ki kaynaÄŸÄ± karÅŸÄ±laÅŸtÄ±r
                plumber_text = text_plumber.get('metin', '')
                alternative_text = text_alternative.get('metin', '')
                
                # Uzun olan metni seÃ§ (genellikle daha doÄŸru)
                if len(plumber_text) > len(alternative_text):
                    final_text = text_plumber.copy()
                    final_text['confidence'] = 'high'
                    final_text['validation'] = 'both_sources'
                else:
                    final_text = text_alternative.copy()
                    final_text['confidence'] = 'high'
                    final_text['validation'] = 'both_sources'
                    
            elif text_plumber:
                final_text = text_plumber.copy()
                final_text['confidence'] = 'medium'
                final_text['validation'] = 'pdfplumber_only'
                
            elif text_alternative:
                final_text = text_alternative.copy()
                final_text['confidence'] = 'medium'
                final_text['validation'] = 'alternative_only'
                
            else:
                final_text = {
                    "sayfa": page_num,
                    "metin": "",
                    "paragraflar": [],
                    "confidence": 'low',
                    "validation": 'no_source'
                }
            
            consensus_texts.append(final_text)
        
        logger.info(f"ğŸ”„ Consensus metin Ã§Ä±karÄ±mÄ±: {len(consensus_texts)} sayfa")
        return consensus_texts
    
    def extract_titles_from_text(self, text_data: List[Dict]) -> Dict[int, str]:
        """Metinden baÅŸlÄ±k Ã§Ä±karÄ±mÄ±"""
        titles = {}
        
        for page_data in text_data:
            page_num = page_data['sayfa']
            paragraphs = page_data.get('paragraflar', [])
            
            # BaÅŸlÄ±k heuristics
            for para in paragraphs:
                if (len(para) < 100 and 
                    len(para) > 10 and 
                    para[0].isupper() and
                    not para.endswith('.') and
                    not para.startswith('â€¢') and
                    not para.isdigit() and
                    'tablo' not in para.lower() and
                    'grafik' not in para.lower()):
                    titles[page_num] = para
                    break
            
            if page_num not in titles:
                titles[page_num] = f"Sayfa {page_num}"
        
        return titles
    
    def parallel_extraction(self) -> Dict[str, Any]:
        """Parallel processing ile tÃ¼m Ã§Ä±karÄ±m iÅŸlemlerini yap"""
        logger.info("ğŸ”„ Parallel extraction baÅŸlatÄ±lÄ±yor...")
        
        results = {}
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            # Parallel tasks
            futures = {
                'text_pdfplumber': executor.submit(self.extract_text_pdfplumber),
                'text_alternative': executor.submit(self.extract_text_alternative),
                'tables': executor.submit(self.extract_tables_pdfplumber),
                'images_pdf2image': executor.submit(self.extract_images_pdf2image),
            }
            
            # SonuÃ§larÄ± topla
            for task_name, future in futures.items():
                try:
                    results[task_name] = future.result(timeout=30)
                except Exception as e:
                    logger.error(f"âŒ {task_name} task hatasÄ±: {e}")
                    results[task_name] = []
        
        # Fallback: pdf2image baÅŸarÄ±sÄ±z olursa pdfplumber kullan
        if not results.get('images_pdf2image'):
            logger.info("ğŸ”„ pdf2image fallback: pdfplumber kullanÄ±lÄ±yor...")
            results['images_pdfplumber'] = self.extract_images_pdfplumber()
        
        return results
    
    def build_final_output(self, extraction_results: Dict[str, Any]) -> List[Dict]:
        """Final JSON Ã§Ä±ktÄ±sÄ±nÄ± oluÅŸtur"""
        
        # Consensus text extraction
        consensus_texts = self.consensus_text_extraction(
            extraction_results.get('text_pdfplumber', []),
            extraction_results.get('text_alternative', [])
        )
        
        # BaÅŸlÄ±k Ã§Ä±karÄ±mÄ±
        titles = self.extract_titles_from_text(consensus_texts)
        
        # GÃ¶rseller (pdf2image Ã¶ncelikli)
        images = extraction_results.get('images_pdf2image') or extraction_results.get('images_pdfplumber', [])
        
        # Sayfa bazlÄ± birleÅŸtirme
        final_pages = []
        
        for page_data in consensus_texts:
            page_num = page_data['sayfa']
            
            # Sayfa iÃ§in tablolarÄ± bul
            page_tables = [t for t in extraction_results.get('tables', []) if t['sayfa'] == page_num]
            
            # Sayfa iÃ§in gÃ¶rseli bul
            page_images = [img for img in images if img['sayfa'] == page_num]
            
            final_page = {
                "sayfa": page_num,
                "baÅŸlÄ±k": titles.get(page_num, f"Sayfa {page_num}"),
                "paragraflar": page_data.get('paragraflar', []),
                "tablolar": [t['data'] for t in page_tables],
                "grafikler": [{
                    "baÅŸlÄ±k": titles.get(page_num, f"Sayfa {page_num}"),
                    "gÃ¶rsel_path": img['gÃ¶rsel_path'],
                    "Ã§Ã¶zÃ¼nÃ¼rlÃ¼k": img.get('Ã§Ã¶zÃ¼nÃ¼rlÃ¼k', 'unknown'),
                    "kaynak": img.get('kaynak', 'unknown'),
                    "Ã§Ä±karÄ±lan_veri": None
                } for img in page_images],
                "confidence": page_data.get('confidence', 'unknown'),
                "validation": page_data.get('validation', 'unknown')
            }
            
            final_pages.append(final_page)
        
        return final_pages
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """Performance raporu oluÅŸtur"""
        total_time = sum(stats['time'] for stats in self.performance_stats.values())
        
        report = {
            "toplam_sÃ¼re": f"{total_time:.2f}s",
            "araÃ§_performansÄ±": {},
            "baÅŸarÄ±_oranÄ±": {},
            "Ã¶neriler": []
        }
        
        for tool, stats in self.performance_stats.items():
            report["araÃ§_performansÄ±"][tool] = {
                "sÃ¼re": f"{stats['time']:.2f}s",
                "baÅŸarÄ±": stats['success'],
                "hata": stats['errors']
            }
            
            success_rate = stats['success'] / (stats['success'] + stats['errors']) if (stats['success'] + stats['errors']) > 0 else 0
            report["baÅŸarÄ±_oranÄ±"][tool] = f"{success_rate:.1%}"
            
            if stats['errors'] > 0:
                report["Ã¶neriler"].append(f"{tool}: {stats['errors']} hata tespit edildi")
        
        return report
    
    def run_hybrid_extraction(self) -> Dict[str, Any]:
        """Ana hybrid extraction pipeline"""
        logger.info("ğŸš€ Hybrid PDF Extraction baÅŸlatÄ±lÄ±yor...")
        
        # Parallel extraction
        extraction_results = self.parallel_extraction()
        
        # Final output
        final_pages = self.build_final_output(extraction_results)
        
        # Performance report
        performance_report = self.generate_performance_report()
        
        # JSON Ã§Ä±ktÄ±sÄ±
        output_data = {
            "pdf_dosyasÄ±": str(self.pdf_path),
            "Ã§Ä±karÄ±m_tarihi": time.strftime("%Y-%m-%d %H:%M:%S"),
            "sayfa_sayÄ±sÄ±": len(final_pages),
            "sayfalar": final_pages,
            "performans_raporu": performance_report,
            "hybrid_extraction": True
        }
        
        # Dosyaya kaydet
        output_file = self.output_dir / "hybrid_extracted_data.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… Hybrid extraction tamamlandÄ±: {output_file}")
        return output_data

# Ana Ã§alÄ±ÅŸtÄ±rma
if __name__ == "__main__":
    pdf_path = "2025_07_16_Haziran AyÄ± BÃ¼tce Dengesi.pdf"
    
    print("ğŸš€ Hybrid PDF Extraction System")
    print("=" * 50)
    
    extractor = HybridPDFExtractor(pdf_path)
    result = extractor.run_hybrid_extraction()
    
    print("\nğŸ“Š Ã–zet:")
    print(f"ğŸ“„ Sayfa sayÄ±sÄ±: {result['sayfa_sayÄ±sÄ±']}")
    print(f"â±ï¸ Toplam sÃ¼re: {result['performans_raporu']['toplam_sÃ¼re']}")
    print(f"ğŸ¯ Hybrid extraction: {result['hybrid_extraction']}")
    
    print("\nğŸ”§ AraÃ§ PerformansÄ±:")
    for tool, perf in result['performans_raporu']['araÃ§_performansÄ±'].items():
        print(f"  {tool}: {perf['sÃ¼re']} - BaÅŸarÄ±: {perf['baÅŸarÄ±']}, Hata: {perf['hata']}")
    
    print(f"\nâœ… Ã‡Ä±ktÄ±: {extractor.output_dir}/hybrid_extracted_data.json")
    print(f"ğŸ–¼ï¸ GÃ¶rseller: {extractor.output_dir}/ klasÃ¶rÃ¼nde") 